[//]: # (Image References)

[image1]: rewards.png "Plot Reward"

# Project 2: Continous Control

In order to solve the Reacher environment (i.e. maximizing the total reward by being in the goal location as many timesteps as possible) I decided to choose the Deep Deterministic Policy Gradient algorithm (`Continuous_Control.ipynb`). Moreover, version 2 was solved by training each of the 20 agents on the continuous control task separately. The agents didn't interact with each other. They share one common Replay Buffer and also one and the same networks. Training is carried out in a sequential manner, i.e. every agent updates the networks one after the other.
As it is common for DDPG there is an Actor and a Critic each comprising a local and a target network. Basically, the actor has the purpose to create an action for a given state. The produced action is fedback into the critic and thus the critic is able to check whether the actor made a good choice for a given state by producing a single state action value.
Local and target network of the actor each are based on an Artificial Neural Network consisting of 3 Linear Layers. The first two of them have ReLu activation, the output layer has a tanh activation. Moreover, the input layer also uses Batch Normalization to avoid overfitting. For mapping the 33 space dimensions to a continuous action of size 4 (comprising the torques for the double jointed arm) the input layer has size 33 x 512, the hidden layer has size 512 x 256 and the final output layer has dimension 256 x 4. On the other hand, the critic is also built of an Artificial Neural Network with 3 Linear Layers. Compared to the actor it has the same configuration except from the hidden layer which additionally takes the action as input (size 516 x 256) and the final output layer of size 256 x 1.
In order to solve the Reacher environment every agent follows a combination of policy evaluation and policy improvement in each iteration. For policy evaluation actions (for all agents) are created by the actor local network based on the current state. These actions are perturbed by Ornstein-Uhlenbeck noise. The amount of noise added to the actions is controlled by the hyperparameter epsilon. Epsilon is initially set to 1.0 and is decreased by a factor of 0.999999 after each single update of the networks. The hyperparamater should model the exploration-exploitation tradeoff. At the beginning there has to be much exploration (thus much noise) while at later time noise is decreased (exploitation) as the agents are more confident in their decisions.
On the other hand policy improvement is built of two phases - a sampling phase and a learning phase. This process is executed by each agent sequentially. Each timestep the shared Replay Buffer (BUFFER_SIZE = 1e6) is extended by important information like state, action, reward, next state and the fact whether the end of the game is reached. By random sampling a certain number of instances (BATCH_SIZE = 128) from this buffer and providing it to the agent the agent tries to improve the model weights of the actor and critic of DDPG based on these experience replay data. Technically, this means carrying out the Backpropagation algorithm with the goal to minimize the loss between target state action values and predicted state action values for the critic. Morevoer, gradient clippling to a normalization value of 1 is applied for training the critic in order to make training more stable.
For training the actor the goal is to maximize the state action value resulting from the critic. The idea is to replicate the DDPG architecture at the beginning and to make a soft update to the weights of the target networks by copying over the corresponding weights of the local networks. The tradeoff between old target parameters and local ones is handled by hyperparameter tau. In the underlying training procedure tau is chosen to be 1e-3. 
I decided to choose a Mean Squared Error Loss for both actor and critic loss. Taking the gradient of the loss w.r.t. model parameters and feeding it back through the network the optimizer (Adam) is finally able to update the weights. In this context the hyperparameter LR_ACTOR is the step size for updating the weights of the actor, LR_CRITIC the equivalent for the critic. These hyperparameters are set to LR_ACTOR = 1e-4 and LR_CRITIC = 1e-3. Finally, at the end of each agent's update noise is reset and epsilon is lowered.
All in all, each iteration of an episode each agent picks an action (policy evaluation), interacts with its environment (thus receiving the next state, reward and the information if the game is done) and learns from experience replay data.
It should be mentioned that learning is carried out every 20th iteration with 10 subsequent updates (i.e. 10 agents contribute to the update). 
In total, the agents play 300 episodes each terminating either after 1000 iterations are reached or if any of the agents finishes the game earlier. The 20 agents are able to solve the environment in 104 episodes (i.e. achieving an average reward of 30.25 for 100 consecutive episodes and over all agents).
The trained model weights are stored in `checkpoint_actor_local.pth` and `checkpoint_critic_local.pth`. The following plot shows the scores during training.

![Plot Reward][image1]

In the future one could even improve performance by e.g. doing a hyperparameter optimization or by investigating other algorithms like e.g. TRPO or D4PG. Moreover, training time could certainly be decreased by using a parallel training procedure of my DDPG (e.g. by using library multiprocessing). Each agent would simultaneously sample from the shared Replay Buffer and calculate the gradient based on copies of the original networks. At the end of each training iteration there would be a barrier for synchronization of all agents and after that each agent would contribute to accumulation of gradients to the original network.